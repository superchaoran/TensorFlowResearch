{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I always find useful to display the shape or the content of the variables to better understand their structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 27)\n",
      "1562484\n",
      "26\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_batches.next()[1].shape)\n",
    "print(len(train_text) // batch_size)\n",
    "print(len(string.ascii_lowercase))\n",
    "print(np.zeros(shape=(2, 4), dtype=np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.290833 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.87\n",
      "================================================================================\n",
      "croeor fsedeiohzlnnebmgiew kerfvsrgsxeyojlyyaqageadvsiydgm fmg knuerbhv cpniqvjl\n",
      "qnuekaa nqgtpxexlnniulbue  mom oteo anyadoeywxfepw aozyq  litj ljomrwkvmvlmh nek\n",
      "wtal tx  iotlgajeniwjegcoo edsivoujll yecqo sghdsze x mhnohct oedi kiuphtqotvemp\n",
      "i esl  ia wq  in jdtawots evib akvscemij kmfoyaqmtrfonocey  iwv dhnz  nttros p z\n",
      "  asauieicwfdpl  bjexwxsz qgjtlvhiyayyisyh jjok dy ygptqs inqo nzfdjpnuv estsjr \n",
      "================================================================================\n",
      "Validation set perplexity: 20.00\n",
      "Average loss at step 100: 2.599098 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.02\n",
      "Validation set perplexity: 10.47\n",
      "Average loss at step 200: 2.250017 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.58\n",
      "Validation set perplexity: 8.98\n",
      "Average loss at step 300: 2.094530 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 400: 1.998736 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 500: 1.928923 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 600: 1.902863 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 700: 1.850783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 800: 1.811186 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 900: 1.825453 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1000: 1.817183 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "tion cain chias of ginsling the romedled the ameryov is to the have of relrains \n",
      "jem to base andsliven the driged the informan fivility of two mifhat and wnick a\n",
      "macts which distuce costing churber the condibs of the sexveld pecinvinetage is \n",
      "versition factiys waved cartieghing caill sichal dayegusare prail hall in to two\n",
      "y precuce of hard in rechikis ristric fnussorporion wender of ald dirgs pochrike\n",
      "================================================================================\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1100: 1.774367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1200: 1.746894 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1300: 1.731863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1400: 1.741996 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1500: 1.732596 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1600: 1.742589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1700: 1.706562 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1800: 1.674079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 1900: 1.648539 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2000: 1.692980 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "ed anch teonectively see five or see nine trex bahering the vircal one nine five\n",
      "by a presules networe drough generoning hee simual the rusercart a harnon a stan\n",
      "halged but postig in a setr force the culfinn butmen elsplieess indibusing stife\n",
      "y longual pro colsendpall he he nusitis that the fromm a math s dinferenting ity\n",
      "w ghen the fering pupser infinite his recompalary induated euschical dosibreed i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2100: 1.684056 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2200: 1.677643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2300: 1.639645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2400: 1.659794 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2500: 1.680889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2600: 1.649497 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2700: 1.653062 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2800: 1.648103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 2900: 1.648330 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3000: 1.648915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "l and bole are the frince that hughty ovace to instanced its one nine three nine\n",
      "eal any sugan of graster and falft in the futiam and hat actors as the mooclated\n",
      "jews and unord sinciteag defferently tuated beloving aprit and more the stawon t\n",
      "mection of sandach end othercl itsede and first in the kultsthy charde and grack\n",
      "quagest of recome and stated bect adva to the peating that monta in theol as ant\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3100: 1.624917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3200: 1.640911 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.639182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3400: 1.666530 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3500: 1.654702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3600: 1.662412 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3700: 1.644536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.641746 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3900: 1.638383 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4000: 1.645368 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "s teres cicter marriagure other puthinz emporow asear platon casilly fross trays\n",
      "quuge his souticient howischatical expline following as for doner liees by to am\n",
      "it of two a highers monches delazem of four mezimizations six armentimoring line\n",
      "ffocuem the lifthal descended actrestris twoszer and only instrutord centralizat\n",
      "ter of spulttday distenty who sound o negimal chultize onlae with the masaim by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4100: 1.627419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.632525 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4300: 1.617535 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4400: 1.603760 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4500: 1.616041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4600: 1.609113 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4700: 1.621612 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4800: 1.630161 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4900: 1.630647 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000: 1.606286 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "beaph or higher on munch be first addoy competies in distorcifot parrolative the\n",
      "jeenhes miar possil comsonded developpes mostic y disd and theremome by that is \n",
      "s of the nate jos one nine four four five the kera seven to unittyr effice edeme\n",
      "r by smitist in the is in come and nahqule birth is aud commersions this ball fr\n",
      " to reparted itchetic a p from not with perack after the betherogence losing to \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5100: 1.602897 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5200: 1.587045 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5300: 1.578843 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.572030 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.564303 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5600: 1.577664 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.565941 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5800: 1.577107 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.568005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6000: 1.539472 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "ven term corries hyperinse nearly seviecty high to fect pretaiane labid u is dak\n",
      "y one nine five eight six two one four the pircistus kb one addary of citionia h\n",
      "pher art attemploys and will hanjan oficia sgeed are of the was averetrialic is \n",
      "ney giving with misculutody so procoom st to marrie titlative nina arear ise vis\n",
      "x nine no five recept that ehomd lajpers bipprates a sufter the cervantes baseo \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.562850 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6200: 1.532259 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.538332 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6400: 1.534852 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6500: 1.551552 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6600: 1.589246 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6700: 1.577858 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6800: 1.597956 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6900: 1.579061 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 7000: 1.574467 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "jone notination haveaborle intolves doures dyctions proce explage but not were m\n",
      "cts wells the sporsulation was usswan its granaday out as bechestable producly t\n",
      "othahoum reporting disfect of a higher one is a ablines ethero privation had for\n",
      "quer of peoples nears made of agged teme came schevally three nine four elise se\n",
      "vers of presiderencoge liets e haveody that three terrodboirity of efberdation o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters  \n",
    "  sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "  sm = tf.concat(1, [im, fm, cm, om])\n",
    "  sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(1, 4, smatmul)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299838 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.11\n",
      "================================================================================\n",
      "fay ibx enar  drhma  hnqqzelepe  xehi asiokfcrczkt ai ksau  dmr nh m  xvdz  npes\n",
      "cczi ee glpiumhe jzfnrloobuxeowckqhseueegwe dhncehoafoteqzcnt    lsweaubanauhy r\n",
      "y dixdgki akerepjpe ecvr pidspyeprfiloh ezqt rgnxeirtjxfy tu www x oe iyd moak y\n",
      "uian gc oeael qsjz m irvfmztcal xcjkndu dron padhealxcrhlkcv  jeptr  aifa s ovah\n",
      "iu qa em oe ox vpneliikyazdi e swu fgbeeb  ax i dnoxwf edevm  dzpvc wzuqkduon xs\n",
      "================================================================================\n",
      "Validation set perplexity: 20.26\n",
      "Average loss at step 100: 2.597871 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.48\n",
      "Validation set perplexity: 11.84\n",
      "Average loss at step 200: 2.266672 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.40\n",
      "Validation set perplexity: 9.17\n",
      "Average loss at step 300: 2.097994 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 400: 2.037933 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 500: 1.983375 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 600: 1.900528 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 700: 1.877608 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 800: 1.872401 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 900: 1.848276 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 1000: 1.850099 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "================================================================================\n",
      "e s specumsss flouss one sy arter strave to bowark pattries encloma efonte fwass\n",
      "rucin beona in a mag hebort hille redection carsodary it istrophicatiply thlouir\n",
      "zers waish as yarth an the symoutery und h on dift lay dupy a u wethination by a\n",
      "ut holparumoutrege dadolociogs merazer than han isforman ockulle seemblous mulin\n",
      "lan one nine four upi tyly twollable of um a commars was not rotes p was sfitsen\n",
      "================================================================================\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1100: 1.805583 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1200: 1.773644 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1300: 1.762076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1400: 1.769336 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1500: 1.754726 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1600: 1.736742 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1700: 1.720509 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1800: 1.692874 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1900: 1.700861 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2000: 1.684115 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "================================================================================\n",
      "x of two while n those booknee liven of their sattime fogally and retugned fonem\n",
      "velia ackige ordose computorkist offectal zink offome gramcles a seven d one of \n",
      "ented time through to i of lon and later expleeming whetwern of ctoully ishinger\n",
      "acker pands but requince line doun ite sampanechy belaked agrees detectish desin\n",
      "dems masentus croduced mocely expreem dif toumation on the ecombenny herrobally \n",
      "================================================================================\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2100: 1.691942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2200: 1.709051 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2300: 1.711278 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2400: 1.686183 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2500: 1.695655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2600: 1.673464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2700: 1.687300 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2800: 1.681263 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2900: 1.679183 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3000: 1.682997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "ruptict ratile courrs as lixt word berico herb a vegnors desmituragks runstipery\n",
      "noatish unfer the dalumary nomisoreter egistiate hersham adstime and two hes as \n",
      "je beging getrs to bit brained can the one two zero zifks of aughers by allimbge\n",
      "ish the ror clieptiah instaters commuters pliemen eussives in the departy to fro\n",
      "ly the approdn servical only showewn one knines the rasiin and the s assoving go\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3100: 1.654959 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3200: 1.640071 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3300: 1.648174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3400: 1.637886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3500: 1.676584 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3600: 1.654893 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3700: 1.654158 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3800: 1.656445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3900: 1.651152 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4000: 1.647028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "houses languaz wel the indefirs back other hmppubler neftly and one nine eight f\n",
      "ce be as from the chuinting a five millian poltsed seven one zero zero zero zero\n",
      "y semizorally from ralier out the dimckion of alatu popustybing to counts the mi\n",
      "for the poller mugising has country examples of hamitary four two seven five one\n",
      "ets has to umick acuisia s of enclo kaft perself libutian in the doundia s rodum\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4100: 1.621355 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4200: 1.617763 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4300: 1.622404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4400: 1.613061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4500: 1.642778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4600: 1.625921 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4700: 1.624567 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4800: 1.609569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4900: 1.621022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5000: 1.613098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "all off to the tech allou alkhor typic eiffel ficturand pereovis will one nine e\n",
      "oques ares aircatusing upote to to its solf complay the ock of all with the brag\n",
      "hanius unioxs rayst to a gagh barngs of freed after other a corpied as all i wor\n",
      "n waparibu it wiselbaw of over n to a fill b hide stund bords mant the blenged t\n",
      "en mistria destance retaltar the from second while vaje mass d city with cord of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5100: 1.592792 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5200: 1.593179 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5300: 1.592581 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.598673 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5500: 1.588570 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5600: 1.561666 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5700: 1.576770 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5800: 1.601654 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5900: 1.581363 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6000: 1.586125 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "nevis of german proteurifioute x new by epchezetleed and with upon gamers yearly\n",
      "men presseencing the euntm of tginm redired thele and chrifence undes or underth\n",
      "menty balk kien monution mids of then counts distinging the mulital mefreals the\n",
      "t wilitzers delians or formbally cublegored the be modiass the hear and fatics t\n",
      "all aircrading to mikia ory dier stote six speellimber mide wept to maics intend\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6100: 1.576415 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6200: 1.589789 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6300: 1.591686 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6400: 1.572414 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6500: 1.558354 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6600: 1.607104 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6700: 1.570288 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6800: 1.584826 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6900: 1.570373 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7000: 1.590415 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "was been of presidentle dovers are agrects work by a of currently p selted built\n",
      "formed are a second deserpinans mercial the uxties orf in a chullers in one nine\n",
      "zer add roukened anymos levels patal from northing onder faut at a part it patiu\n",
      "basing or fourbone three one five one twrate atuallially four eight one gisters \n",
      "monally shappee five he after anticaonary hass some areas bromog of county descr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first adapt the LSTM for a single character input with embeddings. The ``feed_dict`` is unchanged, the embeddings are looked up from the inputs. Note that the output is an array probability for the possible characters, not an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.308185 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.34\n",
      "================================================================================\n",
      "uzprtis oaio skbkhqsioeqkxegteiew rei ynst lpp tn otmnt  oee qx hfflbpkh   bikkr\n",
      "viynno c oaofiveevextdoevsjel fw kee  ftacr mwk  rimeg eioeca auykjeiwfazaitxrpk\n",
      "aoryltmh   t eql ofcwaopgnus ebt xhttrony cpsd eonmvyih h xreem mepbvhivf seepen\n",
      "hexbf utm  yofjtt  rqfjd a utiku dray s kf gnwfp rvedzprdbimo vgao paweer oaofbe\n",
      "gbepdoby dh li enseorbatqdrmibqeana taponnzeffkrex e gu ebmltm aom hsudtm h ohat\n",
      "================================================================================\n",
      "Validation set perplexity: 18.96\n",
      "Average loss at step 100: 2.290958 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.57\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 200: 2.019428 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 300: 1.922145 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 400: 1.868226 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 500: 1.885315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 600: 1.821329 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 700: 1.800218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 800: 1.791239 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 900: 1.785651 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1000: 1.718493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "er indicinecture beackplesencition shanlic as mix for than tairing sink capinuag\n",
      "allowing stable for ore one six nine tik simlarc earding subptzer builine terce \n",
      "y redratiganished duch hum an orging cadaction condislince have turish and fecti\n",
      "mine presim aks as in the firming efed ociemiens it of station in in temple comp\n",
      "gasterne i and more at litin rand frert act for zero eight to rage the intrack s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1100: 1.699397 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1200: 1.735633 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1300: 1.713835 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1400: 1.691044 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1500: 1.689062 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.676918 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1700: 1.708798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1800: 1.673470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1900: 1.682104 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2000: 1.688296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "toms feathanies at histors poit anoftal phate boorkhenge docters of f lks at the\n",
      "m oriand resaptics browsp dow in la makabaya grousp based of a  and the cayence \n",
      "hup wrating suphane in be con smoned or track lax or acterial diffeten have not \n",
      "not wraty sup kming abor of facies about bap fire seven hand that with the other\n",
      "xiesttc phari charterium d actor for arbers it concy abolid maagemmy of the late\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.682177 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2200: 1.652925 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2300: 1.666340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2400: 1.663923 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2500: 1.686373 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2600: 1.664212 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2700: 1.678025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2800: 1.641605 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2900: 1.645346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3000: 1.653721 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "d s relanys beliek daga liveus gored with agale the litin pey the govershardin a\n",
      "ly another is contai gosunity n march externales capia presider prothver iring a\n",
      "ques an appli than supping a soman leg sau icl of entrates conside elecon castur\n",
      " from coundays of out the unoi whan callent in chared save for the gocurity capi\n",
      "on nocal leng gy ducede companigz syme that intholic in bavemury s everdings for\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3100: 1.651961 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3200: 1.647786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3300: 1.633868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3400: 1.634451 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3500: 1.630291 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3600: 1.633820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3700: 1.635285 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3800: 1.626727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3900: 1.619387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4000: 1.625236 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "ones a robies mackood muslaim two two two seven one nine five the set the agonal\n",
      "th one eight seven threen and on seven forts march office the ppt one in the jam\n",
      "thory muchse programe that the indenatifiencis the unites man and were degpipis \n",
      "rese a nalfical ressolt resorcosioned in partionally one nine eight seven zero v\n",
      "noward near brecto hugina but have defill weened ladesy about co linebars clver \n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4100: 1.627746 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4200: 1.607266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4300: 1.601285 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4400: 1.626858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4500: 1.634282 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4600: 1.640463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4700: 1.614647 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4800: 1.599955 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4900: 1.613807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5000: 1.638536 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "kens incluge offers aymons three one five one minaves primation three five to hi\n",
      "dengia sortapic quals thisk the cartharous townet kakes of attactures golementcu\n",
      "jour called off ub carianticular ban ba not gack to the numuldshed player kecapt\n",
      "hers in preated excullectoran match was datantule in the one leter stdittent sba\n",
      "ru of years lecens a accept of ivelitures the jane preaches its cypits of beear \n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 5100: 1.621951 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5200: 1.610618 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5300: 1.576233 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5400: 1.575752 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5500: 1.563717 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5600: 1.590715 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5700: 1.547438 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5800: 1.555498 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5900: 1.570411 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000: 1.537546 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "xysted of burnis three poluber and mian world as chilianded marti those cretured\n",
      "er los messia can with net alge thingtenthen sta tgen befoven and as their each \n",
      "on esercipum work two zero zero two lead enticle in orthero one eight two zero t\n",
      "rights to be praache son promes in one eight one two nine kulad carinias and org\n",
      "ybrace of fard economice object try for a four vers to nine to orders reports of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6100: 1.565291 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6200: 1.577446 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6300: 1.591111 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6400: 1.621289 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6500: 1.615185 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6600: 1.579302 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6700: 1.571465 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6800: 1.557130 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6900: 1.542677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7000: 1.559252 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "z ive but over phil faum prouron homo known sector israelarations instempt gared\n",
      "ch of journational currentained boasses dor saying number birsen usenvers oljage\n",
      "ras an c processaremuria waldard ranticlation made n in vision xilds of and part\n",
      "cal ether destrail his large core roung one five one affere nicc peoplick others\n",
      "x one zero zero sheam nine five net hraddess in the haller based continues or sa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use bigrams as inputs for the training. Here again, the ``feed_dict`` is unchanged, the bigram embeddings are looked up from the inputs. The output of the LSTM is still a probability array of the possible characters (not bigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #print(i.get_shape())\n",
    "    #print(i)\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    #print(logits.get_shape())\n",
    "    #print(tf.concat(0, train_labels).get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.318626 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.62\n",
      "================================================================================\n",
      "frnt plen cbemuayo xw nehdwnmmrfoeajjrundfbz soess  of rp oemd bpmt uhuensnciwoue\n",
      "kzoim gnes vea lae dyutlegoa ufhteneazipedwzl hrn ngvnxhnenfzs dt lxeyx pttkrmwpi\n",
      "eytqip fmoff  weyayoopeloc sfedgtron w u usob sz tehverdknaiar agylpn mueeqllnkq \n",
      "vwk ecltunwysrwhoehnpz waev ahgd rseftxjamityapa nsqteasbjaxykwwernele jyurdayrop\n",
      "zg gr arbedz  tzswvgj iivf dznfhipeenioeox  drcewz w ertagq oc  xmefezo ootmn e o\n",
      "================================================================================\n",
      "Validation set perplexity: 19.26\n",
      "Average loss at step 100: 2.268696 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 9.24\n",
      "Average loss at step 200: 1.968067 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 300: 1.878620 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 400: 1.820958 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 500: 1.754978 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 600: 1.757149 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 700: 1.739477 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 800: 1.723395 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 900: 1.713418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 1000: 1.685530 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "xditions jamps in in in the costs accunction desix four not in first confue scour\n",
      " ling oncontrax ediefernal polities pointed ough the muided a souprence to the co\n",
      "vhuagenzeam year they plandares but debonion ethces with the him and ston is my t\n",
      "jqnal dess that the notics in smage is by a from other he inqsmilaciii s to geora\n",
      "xb mossion unromiult theor presix and advicgoach tapersion of s is bondouble mayr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 1100: 1.692593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1200: 1.693425 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 1300: 1.686072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 1400: 1.658291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1500: 1.649894 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 1600: 1.640422 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 1700: 1.648546 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 1800: 1.668477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1900: 1.647707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 2000: 1.664337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      " years as about langel attiniumneven has of an riarly wollown are semens generain\n",
      "yring and meacome currency were the soongsularrelf the its in beat as gamgest in \n",
      "ues econa of in a off the continentify at the mcture of the reconscrossinging joi\n",
      "qqlcted by between one the chilor country maing extered enumed to necwu ang as en\n",
      "wqae filemation include from aking left her secum a the only wane heary between e\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 2100: 1.644328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2200: 1.665724 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 2300: 1.644030 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 2400: 1.640833 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2500: 1.653393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2600: 1.641727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 2700: 1.626792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 2800: 1.626316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 2900: 1.621325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 3000: 1.641185 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "chron one eight two six sessed bia clubteral boiket planse in urs links ber years\n",
      "aas node plzors sonn of the trettimoos belf coonner praijet joking ture pire syst\n",
      "nberitistotion dictures controver roberro rmember one to inverge in eight eight f\n",
      "xel rough kabbns are achieve ursh resultors in the the cup borting they repation \n",
      "pvers with of badone whose and forning much computers a pare see thomes wear both\n",
      "================================================================================\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 3100: 1.613528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 3200: 1.626295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 3300: 1.625137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3400: 1.618745 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 3500: 1.610458 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3600: 1.627255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 3700: 1.597404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 3800: 1.601761 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 3900: 1.586738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 4000: 1.606099 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "ed similics powerlifon calle steoprolant his bank crize earlianomic volume mother\n",
      "zcompaributa is a stologians of erpyro three in one nine one one three the as fun\n",
      "jnation three est earl name some ieb recordina omying is history of elected buss \n",
      "hwesten complex although after ar populand put reed the used goad party governmen\n",
      "rodying more some c style of von mousts and signedule chies fifti charis humanisa\n",
      "================================================================================\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 4100: 1.619975 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 4200: 1.599225 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 4300: 1.569989 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 4400: 1.594471 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 4500: 1.579077 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 4600: 1.588111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4700: 1.600615 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 4800: 1.594609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 4900: 1.617590 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 5000: 1.626473 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "================================================================================\n",
      "yqirm goast one six eight five sixa player to change under which of to be amber o\n",
      "hylutio  radioaski me populate tokyos nari such browns recopulading neentize but \n",
      "ls  elavaul fooding extent alps life ful and bert wearsly secorro europe amber of\n",
      "vwas sinbotts sungdomagy two foods is eight one seven three two three i simply ia\n",
      "sbumen interecembers two zero zero zero in voice dramen teleweight eachin releads\n",
      "================================================================================\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 5100: 1.582361 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 5200: 1.593797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 5300: 1.564525 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 5400: 1.560896 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 5500: 1.559119 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5600: 1.541576 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5700: 1.577677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5800: 1.562164 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5900: 1.573664 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 6000: 1.533729 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.08\n",
      "================================================================================\n",
      "ee book in fradian under canosts comportune which which succed an also four size \n",
      "fcze the son of sembiol more ship on tourae stapunce faileement world six five ei\n",
      "von be discorp dericently it abattch is decembed five one nine eight two est pres\n",
      "f a sevea fee mo the liberal in version kick to be the use lavigived to as to gra\n",
      "cx one four five zero septexplants without refer which in have great was pany bom\n",
      "================================================================================\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 6100: 1.587103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 6200: 1.580116 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 6300: 1.564127 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 6400: 1.583759 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 6500: 1.571847 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 6600: 1.569144 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 6700: 1.559223 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 6800: 1.570441 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 6900: 1.604115 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 7000: 1.584899 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "rous cross and one nine three nine four six and then aan rimation perief it alloh\n",
      "mt a specialleat evolute mess tumbic control ports s be eight nine nine zero zero\n",
      "xkamen all sartotaphat experor a station posities number usa the langam historia \n",
      "dway grounts through davy of high morethernment of area late germiss reprecished \n",
      "sbusen is and frecoruppating streatests us and order wasity even rans shepook of \n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but the validation perplexity is a bit worst.\n",
    "\n",
    "Let's try the dropout, in the inputs/ouputs only, not between to cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 15000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298126 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "pqqxf yejnpatrgtkig n a ef eesqqs x ricnbwo  ysopipinhyligessovmidi evh jyoyexkgm\n",
      "ekn reee tcr eeeyku xck thexvlaakoe kitrvhedv ybx fnneptntgetcenwaqe av  xeeivqfe\n",
      "mrd  gsote am yanoar llsirthnbrisi pe hqierelloqdjil elni ezjeir o epfka anqukpbj\n",
      "tgbndrke gxeaer  hmgoqna lfegoewe habvlobumvaefrybooepsqraudaghv  n al zhnt ex eq\n",
      "geeopek vsq uvtstvnesaelfmsqm eedritdm ihsi cexleoerlfhx ogv mmejwadl ehnvrnfoev \n",
      "================================================================================\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 100: 2.280247 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 9.18\n",
      "Average loss at step 200: 1.964393 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 300: 1.875598 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 400: 1.825816 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 500: 1.796728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 600: 1.759613 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 700: 1.748190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 800: 1.708901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 900: 1.707065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 1000: 1.694766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "er wordsaoid proposion on in and many a srusing conceducity had duwas the filmmed\n",
      "aft wolding for hand cost experia in the neww cosophickathants begard to be of th\n",
      "ey many my in the for lin parts s a sold d one on five one four an in for bulted \n",
      "nly bruth don allied cast on the atto histor differ a di exapped by first line ki\n",
      "ez ricitder med hit hall limbile since follow infland that a line at presides to \n",
      "================================================================================\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 1100: 1.689166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 1200: 1.683288 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 1300: 1.665567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 1400: 1.668549 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 1500: 1.687136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 1600: 1.679554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1700: 1.654288 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 1800: 1.689088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 1900: 1.687585 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 2000: 1.644402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "d viyoor very notes is struction of a maske one two z webno stry chaernly dibe fi\n",
      "ad correceivork trimgtion objectiv beats kounternothement in accidyed note a one \n",
      "jft one eight zero six one princess measults meas of the rogn one two one six two\n",
      "gp were in one nine aubert is of the brimglises inte it sapplian prety peopley re\n",
      "aois input the recept in crash nine eight seven one nine zero two one zero five a\n",
      "================================================================================\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 2100: 1.640469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 2200: 1.624293 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 2300: 1.661928 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 2400: 1.652177 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2500: 1.630692 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 2600: 1.614931 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 2700: 1.615697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 2800: 1.623502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 2900: 1.605834 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3000: 1.602737 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "================================================================================\n",
      "ritity othe when contight wimptreate and  raility those friel stem the certed the\n",
      "rqtish had the madreministandard tran rave is natived worre seo a high towork act\n",
      "ij espestation of loop piece acture reaktance in the high the was mernal the new \n",
      "ly gened and constimpletedsoilo railu seis the chess the ganqueen accomputer for \n",
      "elne as trancors a up exclude of this be loroddine tection berbehina burki reputu\n",
      "================================================================================\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 3100: 1.630525 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 3200: 1.626879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 3300: 1.613139 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 3400: 1.614485 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 3500: 1.600035 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 3600: 1.573974 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 3700: 1.596677 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 3800: 1.608262 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 3900: 1.618929 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 4000: 1.600635 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "================================================================================\n",
      "ologic of the pogy referenty r beink of on the grace in computer very is hav even\n",
      "lbin billides two kpird walt wilha s kraphy has of breath rivers of may wts celin\n",
      "zj noar two zero zero zero four f war able paliminates of at respecie supro s and\n",
      "qps wa hav turring week ballique importen since as immunicia five six two sints c\n",
      "wn guian difficially in one nine seven zero four stockey call most led hony the s\n",
      "================================================================================\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 4100: 1.612594 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 4200: 1.592421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 4300: 1.586818 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 4400: 1.603068 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4500: 1.601239 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 4600: 1.589360 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 4700: 1.592567 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 4800: 1.606060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 4900: 1.591103 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 5000: 1.601910 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "ry and refer in for oted the directter wilhe films responsia sualdly falget she g\n",
      " zeu lest s the depage singe most directing comes californiu sicks onceir clear a\n",
      "cy that u s comical one nine six word swattorical gaoek policy scribed that the f\n",
      "yjimitton parificify is cumbe as herren one nine nine seven two zero four eight m\n",
      "ying at the wave with he alexander case for it most war wimhty and object was fli\n",
      "================================================================================\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 5100: 1.598785 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 5200: 1.599043 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 5300: 1.592231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 5400: 1.576105 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 5500: 1.581541 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 5600: 1.608370 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 5700: 1.582035 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 5800: 1.577867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 5900: 1.586845 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 6000: 1.594359 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "bk university and purposs are of his lood of univer the oation binitinist love th\n",
      "pqqi playpulls blood economician politicisily over open houslin an polineue this \n",
      "rguer to be discrifed pu andhn quaton is x k is an ogre and the united aitolector\n",
      "pman and the persons and nor politic publoped in one two six three a feature ten \n",
      "wlar sohnly one nine five was a sucqsfirson arcank almont highargoyoes darding sa\n",
      "================================================================================\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 6100: 1.606442 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 6200: 1.587579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 6300: 1.597244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6400: 1.628682 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 6500: 1.632020 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 6600: 1.605319 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 6700: 1.603865 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 6800: 1.595303 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 6900: 1.553782 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 7000: 1.599301 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "kfnks geletary courmilation is round dor by the fation by date greats that three \n",
      "w direg committing on three eight year timegolamaollureas bear any and romoss of \n",
      "rcohnisties artilianity adjamedade one zero four five nine five and multildall in\n",
      "ez called the most other and lied and severo withing a emege bethere of skingdom \n",
      "hman so nevikely tenron one of shoor with with grains five five s blaht gre fire \n",
      "================================================================================\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 7100: 1.594116 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 7200: 1.586654 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 7300: 1.599458 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 7400: 1.596856 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 7500: 1.587322 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 7600: 1.576862 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 7700: 1.583297 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 7800: 1.603094 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 7900: 1.614141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 8000: 1.602506 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "sgartrated the basissment the sausepta common that more sanceny at locao two two \n",
      "jbaxided the using pan states in the sufficial subset following aoarder lobed to \n",
      "dqualial as as can presidence rom recunitis divisturb singe stifying on the forma\n",
      "ko grouoanical adducturre they idative jested first an operals ruls clibies the t\n",
      "vlege four safloup many c one two two six which states the rappearly states of th\n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 8100: 1.579029 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 8200: 1.580867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 8300: 1.597957 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 8400: 1.590833 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 8500: 1.606646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 8600: 1.609847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 8700: 1.600009 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 8800: 1.608850 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 8900: 1.587612 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 9000: 1.595045 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "by pry on the heard off two zero zero four sever peosmct and other had a lawn beg\n",
      "mtirt cai the milition afterections way service h estate bal the charge it wesy a\n",
      "zlist counted fidonetice meets in the genestick  de and used come to they face to\n",
      "hferate the gen posses come creen aith the almost latting and one nine nine five \n",
      "dzer decembel life for however pock american can position fbte the five zero bc u\n",
      "================================================================================\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 9100: 1.600626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 9200: 1.619675 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 9300: 1.613638 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 9400: 1.592396 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 9500: 1.605951 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 9600: 1.605210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 9700: 1.606541 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 9800: 1.608262 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 9900: 1.571837 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 10000: 1.589949 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "================================================================================\n",
      "bg kgb signer bens hour this consefs artian is also he den to euserier months men\n",
      "pxtely short or kingoling their today versional gold included eluision who el par\n",
      "ill separate the one or they the usaints and five nine six the eight one si posit\n",
      "jects muss in when bayidge mounted to trys mada systems of provin side prison of \n",
      "wk rates orbitzrhu itself or the with population advancess in one nine systemed i\n",
      "================================================================================\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 10100: 1.603927 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 10200: 1.597886 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 10300: 1.597320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 10400: 1.602001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 10500: 1.613696 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 10600: 1.567307 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 10700: 1.571369 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 10800: 1.586270 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 10900: 1.600734 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 11000: 1.574033 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "rfection at essentil has of the mane a fey which and century be vect lection ther\n",
      "zero sings in university with he cslilinets holic like in the couverments also em\n",
      "mformed used amosentients solve partiii evening one nine six seven eyess groic cw\n",
      "f the five zero nine eight inflight keing of lings lang bearning published includ\n",
      " julible a blangey to colless religame in aising the used into imparter from iii \n",
      "================================================================================\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 11100: 1.559970 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 11200: 1.563666 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 11300: 1.553867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 11400: 1.563165 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 11500: 1.567523 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11600: 1.539310 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 11700: 1.537049 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 11800: 1.558857 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 11900: 1.549654 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 12000: 1.535067 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "rum two two mk an eopic parts on a to early walking of grezslan analysis an over \n",
      "cp metari familited into radirzbons crostly blatgrous movieuiding  drung results \n",
      "wzsr exclusing gif theres year appry and the shis eight s of one th given affinde\n",
      "kv ave renergy and could would a adojt informintled form countelling internation \n",
      "bju dad in plays a and in the main ats frede and serves thereor of atmas that tel\n",
      "================================================================================\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 12100: 1.535230 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 12200: 1.564179 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 12300: 1.550840 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 12400: 1.583935 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 12500: 1.569095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 12600: 1.553721 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 12700: 1.555214 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 12800: 1.569537 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 12900: 1.593961 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 13000: 1.565049 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "================================================================================\n",
      "yon reputates by films of hbc fail case fighties to emperature the mechanient dis\n",
      "sburgh party soverac tralt amination pa perporart of the gsm a definal changes to\n",
      "vd language party panches became rule plation act deport feestian a and heimple t\n",
      "mws hedly end mence an centra specinglean that the tems it and refuude if during \n",
      "cerm scale light as the chip in the from the chare later papeagoard the hip nobel\n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 13100: 1.562145 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 13200: 1.595689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 13300: 1.582578 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 13400: 1.582816 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 13500: 1.601970 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 13600: 1.581185 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 13700: 1.558878 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 13800: 1.536438 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 13900: 1.566373 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 14000: 1.557879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "tain chat person consuman there iresad to a time untry g as volumed speciely is f\n",
      "qnharger over the capelean saxonaged to claimely eight eight zero though membersh\n",
      "lneswrian capable are splake each and opeach autations by the starte this eners i\n",
      "bq for on one nine card suartain to be and a miles phils flow is parliament and a\n",
      "pc became of the less british defenting and station of higher has college shorche\n",
      "================================================================================\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 14100: 1.575171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 14200: 1.579216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 14300: 1.570691 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 14400: 1.581703 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 14500: 1.614355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 14600: 1.589798 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 14700: 1.609144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 14800: 1.591678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 14900: 1.585426 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 15000: 1.575869 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "nking may or pvlace d becomes front of the erudo usually the characters is assuma\n",
      "ythodity may drockmulath apposts as one nine and one one eight is assamerics trea\n",
      "affar whiler others arehave weighnu the lives carrieved by mdi personal some a s \n",
      "vcs teudian existly a relieved moszed from modificatift bic vense uss or mumving \n",
      "dt great mean soneel someter of gusing treations arabloy small of the two popular\n",
      "================================================================================\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 15100: 1.541149 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 15200: 1.566813 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 15300: 1.532356 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 15400: 1.543155 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 15500: 1.507615 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 15600: 1.522672 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 15700: 1.511698 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 15800: 1.500859 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 15900: 1.522362 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 16000: 1.530336 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "vh and swigs archeliberal oncerer stemphin of will script is new priving for arch\n",
      "rroup the demakers of outtided by factive hea are seems contain one zero zero mun\n",
      "k gdp york marj recode you remaid nfrom first jointe was poumanuff urbitatnesbign\n",
      "was faro begilli beconstance and even chhdles tical differ everian five zero zero\n",
      "gth kzed as the paremovent the suppoiste winner among from which during to the wo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 16100: 1.521004 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 16200: 1.493716 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 16300: 1.473555 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 16400: 1.513569 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 16500: 1.525944 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 16600: 1.519266 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 16700: 1.559318 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 16800: 1.506654 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 16900: 1.522093 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 17000: 1.533853 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "dvants parliames had lices in eqavt ered by period of the two est be as exten on \n",
      "ar it in the following politor kr exceptic obcyber ush in the evident for the bel\n",
      "whitle fire stribute status by statucularly sa first among chana highly subsigns \n",
      "rm sappistno briod taxation he winning oodia and maintent five two zero zero four\n",
      "kkaily hold the free and with some and excepts for b is an electused places on hi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 17100: 1.516479 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 17200: 1.543954 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 17300: 1.546927 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 17400: 1.583426 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 17500: 1.566156 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 17600: 1.583906 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 17700: 1.572832 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 17800: 1.553987 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 17900: 1.556987 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 18000: 1.523365 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "ected novel jesulbers if the body s dia near weekmaking norway ship main in prote\n",
      "multance willaciv sologically was until release theory of the automital transity \n",
      " zero three her to the resultural variabil in the from guu other laboer but from \n",
      "kqd par successfultal me labov one two four four n fember one nine one seven five\n",
      "e their m to escapital pionary buildropsage tribat activity tromigrament shoock b\n",
      "================================================================================\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 18100: 1.518369 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 18200: 1.539648 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 18300: 1.541380 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 18400: 1.570286 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 18500: 1.567218 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 18600: 1.572936 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 18700: 1.563469 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 18800: 1.569552 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 18900: 1.551577 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 19000: 1.596760 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "jsee precent due traditions the used into the commonnes hung also love john and b\n",
      "vvd comminance wyydecentry zero sevenzy regarianlant lives fallowings shared as t\n",
      "yous seight nine eight zero zero zero the cobation of the taga year agains closio\n",
      "yof and telpperacy seemes last cloymal the other pip with one nine talval new tog\n",
      "refen s paitor y an one eight three one c in jamary asten to forces b is and exce\n",
      "================================================================================\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 19100: 1.577356 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 19200: 1.551240 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 19300: 1.560418 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 19400: 1.533430 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 19500: 1.534519 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 19600: 1.542783 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 19700: 1.556842 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 19800: 1.536756 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 19900: 1.541107 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 20000: 1.516385 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "in german additional of the pri leastry yoff one nine th nine six zero zero zero \n",
      "tding one nine eight aminuydma that thappe for to means cascendent a six one one \n",
      "pxt the lived in entsrisp to one four three two three eight through the south pri\n",
      "yddline siggrand importarian murders abions d equal coo paunt doroard prious this\n",
      "milty thimes who in one five eight one green the never literad enconseral councid\n",
      "================================================================================\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 20100: 1.522269 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 20200: 1.524450 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 20300: 1.544591 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 20400: 1.548451 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 20500: 1.543598 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 20600: 1.514661 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 20700: 1.501935 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 20800: 1.524286 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 20900: 1.519134 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 21000: 1.518235 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.94\n",
      "================================================================================\n",
      "yl victorh at subtry millions ki pointed to discom onalysis namephys organing son\n",
      "wthose cuski one nine four six one eight seven two zero zero nine zero eight sola\n",
      "gius then repaces advech final my ar a public from keepublished several ree a sco\n",
      "pne secret brettedestite is to serfear viennilities that same on the name primed \n",
      "ivers one of functmon indical for cathit l externation of the redicity year bapus\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1],\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1],\n",
    "                keep_prob_sample: 1.0\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with more steps, the final perplexity is not better. Since I do not know what to expect, and since I do not see any obvious issue (the perplexity being consistent), I'm stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Unfortunately I did not have time to work on this problem in the timeframe of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
